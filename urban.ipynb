{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119043c8-1e48-45e2-a3c9-4ff237106983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import glob\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Column resolution (robust to schema variants)\n",
    "# -----------------------------\n",
    "CANONICAL_COLS = {\n",
    "    \"placekey\": [\"PLACEKEY\", \"placekey\", \"SAFEGRAPH_PLACE_ID\", \"safegraph_place_id\", \"PLACE_ID\", \"place_id\"],\n",
    "    \"visits\": [\"RAW_VISIT_COUNTS\", \"raw_visit_counts\", \"VISIT_COUNTS\", \"visit_counts\"],\n",
    "    \"state\": [\"REGION\", \"region\", \"STATE\", \"state\"],\n",
    "    \"lat\": [\"LATITUDE\", \"latitude\", \"LAT\", \"lat\"],\n",
    "    \"lon\": [\"LONGITUDE\", \"longitude\", \"LON\", \"lon\"],\n",
    "    \"category\": [\n",
    "        \"TOP_CATEGORY\", \"top_category\",\n",
    "        \"SUB_CATEGORY\", \"sub_category\",\n",
    "        \"NAICS_DESCRIPTION\", \"naics_description\",\n",
    "        \"CATEGORY\", \"category\",\n",
    "        \"BRANDS\", \"brands\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def _pick_first_existing(df_cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "    s = set(df_cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    # try case-insensitive match\n",
    "    upper_map = {c.upper(): c for c in df_cols}\n",
    "    for c in candidates:\n",
    "        if c.upper() in upper_map:\n",
    "            return upper_map[c.upper()]\n",
    "    return None\n",
    "\n",
    "\n",
    "def resolve_columns(header_cols: List[str]) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Return mapping: canonical -> actual column name (or None).\"\"\"\n",
    "    mapping = {}\n",
    "    for k, cand in CANONICAL_COLS.items():\n",
    "        mapping[k] = _pick_first_existing(header_cols, cand)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def read_csv_header(path: str) -> List[str]:\n",
    "    return list(pd.read_csv(path, nrows=0).columns)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # all in degrees\n",
    "    R = 6371.0\n",
    "    lat1 = np.deg2rad(lat1); lon1 = np.deg2rad(lon1)\n",
    "    lat2 = np.deg2rad(lat2); lon2 = np.deg2rad(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "def list_monthly_files(data_dir: str, year: int) -> Dict[int, str]:\n",
    "    pat = os.path.join(data_dir, f\"{year}-??--*.csv\")\n",
    "    files = sorted(glob.glob(pat))\n",
    "    month_map = {}\n",
    "    for f in files:\n",
    "        m = re.search(rf\"{year}-(\\d\\d)--\", os.path.basename(f))\n",
    "        if not m:\n",
    "            continue\n",
    "        month = int(m.group(1))\n",
    "        month_map[month] = f\n",
    "    return dict(sorted(month_map.items(), key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Core pipeline steps\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class PanelResult:\n",
    "    panel: pd.DataFrame\n",
    "    table2: pd.DataFrame\n",
    "\n",
    "\n",
    "def build_panel_from_monthly_csvs(\n",
    "    data_dir: str,\n",
    "    state: str,\n",
    "    year: int,\n",
    "    chunksize: int = 200_000,\n",
    ") -> pd.DataFrame:\n",
    "    month_files = list_monthly_files(data_dir, year)\n",
    "    if not month_files:\n",
    "        raise FileNotFoundError(f\"No monthly files found for {year} in: {data_dir}\")\n",
    "\n",
    "    print(f\"Found {len(month_files)} monthly files for {year}:\")\n",
    "    for mm, fp in month_files.items():\n",
    "        print(f\"  {mm:02d}: {os.path.basename(fp)}\")\n",
    "\n",
    "    # sniff header from first file\n",
    "    header_cols = read_csv_header(next(iter(month_files.values())))\n",
    "    colmap = resolve_columns(header_cols)\n",
    "\n",
    "    # required\n",
    "    if colmap[\"placekey\"] is None or colmap[\"visits\"] is None:\n",
    "        raise ValueError(\n",
    "            \"Could not find required columns for placekey/visits.\\n\"\n",
    "            f\"Resolved mapping: {colmap}\\n\"\n",
    "            \"Please open one CSV header and add the correct aliases to CANONICAL_COLS.\"\n",
    "        )\n",
    "\n",
    "    # optional but strongly preferred\n",
    "    st_col = colmap[\"state\"]\n",
    "    cat_col = colmap[\"category\"]\n",
    "    lat_col = colmap[\"lat\"]\n",
    "    lon_col = colmap[\"lon\"]\n",
    "\n",
    "    # We will read minimal columns (case-insensitive usecols callable)\n",
    "    needed_upper = {colmap[\"placekey\"].upper(), colmap[\"visits\"].upper()}\n",
    "    if st_col:  needed_upper.add(st_col.upper())\n",
    "    if cat_col: needed_upper.add(cat_col.upper())\n",
    "    if lat_col: needed_upper.add(lat_col.upper())\n",
    "    if lon_col: needed_upper.add(lon_col.upper())\n",
    "\n",
    "    def usecols(c):\n",
    "        return c.upper() in needed_upper\n",
    "\n",
    "    # store monthly visit series\n",
    "    month_series: Dict[int, pd.Series] = {}\n",
    "    # store meta per placekey\n",
    "    meta = {}\n",
    "\n",
    "    for mm, fp in month_files.items():\n",
    "        print(f\"\\nReading month {mm:02d}: {fp}\")\n",
    "        acc = {}\n",
    "        reader = pd.read_csv(fp, chunksize=chunksize, usecols=usecols, low_memory=False)\n",
    "\n",
    "        for chunk in reader:\n",
    "            # normalize column names (keep originals but access via resolved)\n",
    "            pk = colmap[\"placekey\"]\n",
    "            vc = colmap[\"visits\"]\n",
    "\n",
    "            if st_col:\n",
    "                # state filter\n",
    "                chunk_state = chunk[st_col].astype(\"string\")\n",
    "                chunk = chunk.loc[chunk_state.str.upper() == state.upper()]\n",
    "                if chunk.empty:\n",
    "                    continue\n",
    "\n",
    "            # clean core columns\n",
    "            chunk[pk] = chunk[pk].astype(\"string\")\n",
    "            chunk[vc] = pd.to_numeric(chunk[vc], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            # accumulate visits per placekey within this month\n",
    "            grp = chunk.groupby(pk, dropna=False)[vc].sum()\n",
    "            for k, v in grp.items():\n",
    "                if pd.isna(k):\n",
    "                    continue\n",
    "                k = str(k)\n",
    "                acc[k] = acc.get(k, 0.0) + float(v)\n",
    "\n",
    "            # meta: take first-seen non-missing values\n",
    "            if cat_col or lat_col or lon_col:\n",
    "                sub = chunk[[c for c in [pk, cat_col, lat_col, lon_col] if c is not None]].copy()\n",
    "                sub = sub.drop_duplicates(subset=[pk])\n",
    "                for _, row in sub.iterrows():\n",
    "                    k = row.get(pk, None)\n",
    "                    if pd.isna(k):\n",
    "                        continue\n",
    "                    k = str(k)\n",
    "                    if k in meta:\n",
    "                        continue\n",
    "                    meta[k] = {}\n",
    "                    if cat_col:\n",
    "                        meta[k][\"poi_category\"] = row.get(cat_col, pd.NA)\n",
    "                    if lat_col:\n",
    "                        meta[k][\"latitude\"] = row.get(lat_col, pd.NA)\n",
    "                    if lon_col:\n",
    "                        meta[k][\"longitude\"] = row.get(lon_col, pd.NA)\n",
    "\n",
    "        s = pd.Series(acc, name=f\"m{mm:02d}\", dtype=\"float64\")\n",
    "        month_series[mm] = s\n",
    "        print(f\"  accumulated POIs: {len(s):,}\")\n",
    "\n",
    "    # build wide panel\n",
    "    all_keys = pd.Index(sorted(set().union(*[set(s.index) for s in month_series.values()])))\n",
    "    panel = pd.DataFrame(index=all_keys)\n",
    "\n",
    "    for mm in range(1, 13):\n",
    "        col = f\"m{mm:02d}\"\n",
    "        if mm in month_series:\n",
    "            panel[col] = month_series[mm].reindex(all_keys).fillna(0.0).astype(\"float64\")\n",
    "        else:\n",
    "            panel[col] = 0.0\n",
    "\n",
    "    # meta columns\n",
    "    meta_df = pd.DataFrame.from_dict(meta, orient=\"index\")\n",
    "    panel = panel.join(meta_df, how=\"left\")\n",
    "\n",
    "    # clean meta types\n",
    "    if \"poi_category\" in panel.columns:\n",
    "        panel[\"poi_category\"] = panel[\"poi_category\"].astype(\"string\").fillna(\"Unknown\")\n",
    "    else:\n",
    "        panel[\"poi_category\"] = \"Unknown\"\n",
    "\n",
    "    for c in [\"latitude\", \"longitude\"]:\n",
    "        if c in panel.columns:\n",
    "            panel[c] = pd.to_numeric(panel[c], errors=\"coerce\")\n",
    "        else:\n",
    "            panel[c] = np.nan\n",
    "\n",
    "    # derived targets\n",
    "    mcols = [f\"m{mm:02d}\" for mm in range(1, 13)]\n",
    "    panel[\"avg_monthly_visits\"] = panel[mcols].mean(axis=1)\n",
    "    panel[\"min_monthly_visits\"] = panel[mcols].min(axis=1)\n",
    "    panel[\"max_monthly_visits\"] = panel[mcols].max(axis=1)\n",
    "    eps = 1e-9\n",
    "    panel[\"seasonality_index\"] = (\n",
    "        (panel[\"max_monthly_visits\"] - panel[\"min_monthly_visits\"]) /\n",
    "        (panel[\"max_monthly_visits\"] + panel[\"min_monthly_visits\"] + eps)\n",
    "    )\n",
    "\n",
    "    # distance to \"state center\" (mean lat/lon over POIs with coords)\n",
    "    lat_mu = panel[\"latitude\"].mean(skipna=True)\n",
    "    lon_mu = panel[\"longitude\"].mean(skipna=True)\n",
    "    panel[\"dist_to_center_km\"] = haversine_km(panel[\"latitude\"], panel[\"longitude\"], lat_mu, lon_mu)\n",
    "\n",
    "    panel.index.name = \"placekey\"\n",
    "    return panel\n",
    "\n",
    "\n",
    "def make_fig2_visits_by_category(panel: pd.DataFrame, out_path: str, topk: int = 8):\n",
    "    ensure_dir(os.path.dirname(out_path))\n",
    "    df = panel.copy()\n",
    "    # choose top categories by POI count\n",
    "    top_cats = df[\"poi_category\"].value_counts().head(topk).index.tolist()\n",
    "\n",
    "    data_by_cat = []\n",
    "    for cat in top_cats:\n",
    "        v = df.loc[df[\"poi_category\"] == cat, \"avg_monthly_visits\"].astype(float)\n",
    "        # log-scale plot: keep positive; clamp 0 to 1 for visualization\n",
    "        v = v.clip(lower=1.0)\n",
    "        data_by_cat.append(v.values)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(data_by_cat, tick_labels=top_cats, showfliers=False)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"POI category (top by count)\")\n",
    "    ax.set_ylabel(\"Average monthly visits (log scale)\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def make_fig3_seasonality_by_category(panel: pd.DataFrame, out_path: str, topk: int = 8):\n",
    "    ensure_dir(os.path.dirname(out_path))\n",
    "    df = panel.copy()\n",
    "    top_cats = df[\"poi_category\"].value_counts().head(topk).index.tolist()\n",
    "\n",
    "    data_by_cat = []\n",
    "    for cat in top_cats:\n",
    "        v = df.loc[df[\"poi_category\"] == cat, \"seasonality_index\"].astype(float)\n",
    "        v = v.clip(lower=0.0, upper=1.0)\n",
    "        data_by_cat.append(v.values)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(data_by_cat, tick_labels=top_cats, showfliers=False)\n",
    "    ax.set_xlabel(\"POI category (top by count)\")\n",
    "    ax.set_ylabel(r\"Seasonality index $S_i$\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def make_fig4_cluster_centroids(panel: pd.DataFrame, out_path: str, k: int = 4):\n",
    "    ensure_dir(os.path.dirname(out_path))\n",
    "    mcols = [f\"m{mm:02d}\" for mm in range(1, 13)]\n",
    "    X = panel[mcols].to_numpy(dtype=float)\n",
    "\n",
    "    # standardize per POI by its mean to highlight within-year pattern\n",
    "    mu = X.mean(axis=1, keepdims=True)\n",
    "    Xs = X / (mu + 1e-9)\n",
    "\n",
    "    # keep only reasonable rows (avoid all-zero)\n",
    "    mask = np.isfinite(Xs).all(axis=1) & (mu.flatten() > 0)\n",
    "    Xs2 = Xs[mask]\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "    km.fit(Xs2)\n",
    "    C = km.cluster_centers_  # k x 12\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    months = np.arange(1, 13)\n",
    "    for i in range(k):\n",
    "        ax.plot(months, C[i], marker=\"o\", label=f\"Cluster {i+1}\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Standardized monthly visits\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def fit_static_predictor_and_importance(panel: pd.DataFrame, out_path: str):\n",
    "    \"\"\"\n",
    "    Predict log1p(avg_monthly_visits) from:\n",
    "      - poi_category (categorical)\n",
    "      - latitude, longitude, dist_to_center_km (numeric)\n",
    "    Then permutation importance on ORIGINAL feature columns (safe, no mismatch).\n",
    "    \"\"\"\n",
    "    ensure_dir(os.path.dirname(out_path))\n",
    "    df = panel.copy()\n",
    "\n",
    "    # features\n",
    "    X = df[[\"poi_category\", \"latitude\", \"longitude\", \"dist_to_center_km\"]].copy()\n",
    "    X[\"poi_category\"] = X[\"poi_category\"].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "    # numeric cleanup\n",
    "    for c in [\"latitude\", \"longitude\", \"dist_to_center_km\"]:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X = X.fillna({ \"latitude\": X[\"latitude\"].median(),\n",
    "                   \"longitude\": X[\"longitude\"].median(),\n",
    "                   \"dist_to_center_km\": X[\"dist_to_center_km\"].median() })\n",
    "\n",
    "    y = np.log1p(df[\"avg_monthly_visits\"].astype(float))\n",
    "\n",
    "    # model\n",
    "    cat_cols = [\"poi_category\"]\n",
    "    num_cols = [\"latitude\", \"longitude\", \"dist_to_center_km\"]\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    model = HistGradientBoostingRegressor(random_state=42)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    r = permutation_importance(pipe, X_test, y_test, n_repeats=5, random_state=42)\n",
    "    imp = pd.Series(r.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.barh(imp.index[::-1], imp.values[::-1])\n",
    "    ax.set_xlabel(\"Permutation importance (relative)\")\n",
    "    ax.set_ylabel(\"Input feature\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def build_time_ordered_table2(panel: pd.DataFrame, out_csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Table 2: time-ordered evaluation (train months<=10, test months>=11).\n",
    "    Two targets:\n",
    "      - Visit intensity: log1p(visits_t)\n",
    "      - Seasonality proxy: standardized visits_t / (mean_train_per_poi)\n",
    "    Models: Ridge, RandomForest, HistGBDT\n",
    "    \"\"\"\n",
    "    ensure_dir(os.path.dirname(out_csv_path))\n",
    "    mcols = [f\"m{mm:02d}\" for mm in range(1, 13)]\n",
    "\n",
    "    # build long format\n",
    "    base = panel.reset_index()[[\"placekey\", \"poi_category\", \"latitude\", \"longitude\", \"dist_to_center_km\"] + mcols].copy()\n",
    "    base[\"poi_category\"] = base[\"poi_category\"].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "    # mean over TRAIN months (1..10) per POI for standardization (avoid leakage)\n",
    "    train_months = [f\"m{mm:02d}\" for mm in range(1, 11)]\n",
    "    base[\"mu_train\"] = base[train_months].mean(axis=1).astype(float)\n",
    "\n",
    "    rows = []\n",
    "    for t in range(4, 13):  # need 3 lags\n",
    "        mt = f\"m{t:02d}\"\n",
    "        m1 = f\"m{t-1:02d}\"\n",
    "        m2 = f\"m{t-2:02d}\"\n",
    "        m3 = f\"m{t-3:02d}\"\n",
    "\n",
    "        tmp = base[[\"placekey\", \"poi_category\", \"latitude\", \"longitude\", \"dist_to_center_km\", \"mu_train\", mt, m1, m2, m3]].copy()\n",
    "        tmp[\"month\"] = t\n",
    "        tmp[\"lag1\"] = np.log1p(tmp[m1].astype(float))\n",
    "        tmp[\"lag2\"] = np.log1p(tmp[m2].astype(float))\n",
    "        tmp[\"lag3\"] = np.log1p(tmp[m3].astype(float))\n",
    "\n",
    "        tmp[\"y_intensity\"] = np.log1p(tmp[mt].astype(float))\n",
    "        tmp[\"y_seasonality\"] = (tmp[mt].astype(float) / (tmp[\"mu_train\"].astype(float) + 1e-9))\n",
    "\n",
    "        rows.append(tmp[[\"placekey\", \"poi_category\", \"latitude\", \"longitude\", \"dist_to_center_km\", \"month\", \"lag1\", \"lag2\", \"lag3\", \"y_intensity\", \"y_seasonality\"]])\n",
    "\n",
    "    longdf = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # split time-ordered\n",
    "    train_df = longdf[longdf[\"month\"] <= 10].copy()\n",
    "    test_df  = longdf[longdf[\"month\"] >= 11].copy()\n",
    "\n",
    "    # features\n",
    "    feat_cols_cat = [\"poi_category\", \"month\"]\n",
    "    feat_cols_num = [\"latitude\", \"longitude\", \"dist_to_center_km\", \"lag1\", \"lag2\", \"lag3\"]\n",
    "\n",
    "    X_train = train_df[feat_cols_cat + feat_cols_num].copy()\n",
    "    X_test  = test_df[feat_cols_cat + feat_cols_num].copy()\n",
    "\n",
    "    # clean numeric\n",
    "    for c in feat_cols_num:\n",
    "        X_train[c] = pd.to_numeric(X_train[c], errors=\"coerce\")\n",
    "        X_test[c]  = pd.to_numeric(X_test[c], errors=\"coerce\")\n",
    "    # fill missing\n",
    "    for c in [\"latitude\", \"longitude\", \"dist_to_center_km\"]:\n",
    "        med = X_train[c].median()\n",
    "        X_train[c] = X_train[c].fillna(med)\n",
    "        X_test[c]  = X_test[c].fillna(med)\n",
    "    for c in [\"lag1\", \"lag2\", \"lag3\"]:\n",
    "        X_train[c] = X_train[c].fillna(0.0)\n",
    "        X_test[c]  = X_test[c].fillna(0.0)\n",
    "\n",
    "    # categorical clean\n",
    "    for c in feat_cols_cat:\n",
    "        X_train[c] = X_train[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        X_test[c]  = X_test[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), feat_cols_cat),\n",
    "            (\"num\", \"passthrough\", feat_cols_num),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "        \"HistGBDT\": HistGradientBoostingRegressor(random_state=42),\n",
    "    }\n",
    "\n",
    "    def eval_one(y_train, y_test, target_name: str) -> List[dict]:\n",
    "        out = []\n",
    "        for model_name, model in models.items():\n",
    "            pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            pred = pipe.predict(X_test)\n",
    "            rmse = math.sqrt(mean_squared_error(y_test, pred))\n",
    "            r2 = r2_score(y_test, pred)\n",
    "            out.append({\"Model\": model_name, \"Target\": target_name, \"RMSE\": rmse, \"R2\": r2})\n",
    "        return out\n",
    "\n",
    "    res = []\n",
    "    res += eval_one(train_df[\"y_intensity\"].to_numpy(), test_df[\"y_intensity\"].to_numpy(), \"Visit intensity (log1p)\")\n",
    "    res += eval_one(train_df[\"y_seasonality\"].to_numpy(), test_df[\"y_seasonality\"].to_numpy(), \"Seasonality proxy (v/mu_train)\")\n",
    "\n",
    "    table2 = pd.DataFrame(res)\n",
    "    table2.to_csv(out_csv_path, index=False)\n",
    "    return table2\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "    data_dir: str,\n",
    "    state: str = \"VA\",\n",
    "    year: int = 2021,\n",
    "    chunksize: int = 200_000,\n",
    "    topk_categories: int = 8,\n",
    "    k_clusters: int = 4,\n",
    ") -> PanelResult:\n",
    "    ensure_dir(\"out\")\n",
    "    ensure_dir(\"figs\")\n",
    "\n",
    "    panel = build_panel_from_monthly_csvs(data_dir=data_dir, state=state, year=year, chunksize=chunksize)\n",
    "\n",
    "    out_panel = os.path.join(\"out\", f\"panel_{state}_{year}.csv.gz\")\n",
    "    panel.to_csv(out_panel, compression=\"gzip\")\n",
    "    print(f\"\\nSaved panel: {out_panel} (rows={len(panel):,})\")\n",
    "\n",
    "    # Figures (your fig1 already exists; we generate 2-5 here)\n",
    "    make_fig2_visits_by_category(panel, os.path.join(\"figs\", \"fig2_visits_by_poi_type.png\"), topk=topk_categories)\n",
    "    print(\"Saved figs/fig2_visits_by_poi_type.png\")\n",
    "\n",
    "    make_fig3_seasonality_by_category(panel, os.path.join(\"figs\", \"fig3_seasonality_by_poi_type.png\"), topk=topk_categories)\n",
    "    print(\"Saved figs/fig3_seasonality_by_poi_type.png\")\n",
    "\n",
    "    make_fig4_cluster_centroids(panel, os.path.join(\"figs\", \"fig4_cluster_centroids.png\"), k=k_clusters)\n",
    "    print(\"Saved figs/fig4_cluster_centroids.png\")\n",
    "\n",
    "    fit_static_predictor_and_importance(panel, os.path.join(\"figs\", \"fig5_feature_importance.png\"))\n",
    "    print(\"Saved figs/fig5_feature_importance.png\")\n",
    "\n",
    "    # Table 2\n",
    "    table2 = build_time_ordered_table2(panel, os.path.join(\"out\", \"table2_predictive_performance.csv\"))\n",
    "    print(\"Saved out/table2_predictive_performance.csv\")\n",
    "\n",
    "    return PanelResult(panel=panel, table2=table2)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI - safe with Jupyter\n",
    "# -----------------------------\n",
    "def cli_main(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing monthly CSVs like 2021-01--*.csv\")\n",
    "    parser.add_argument(\"--state\", default=\"VA\")\n",
    "    parser.add_argument(\"--year\", type=int, default=2021)\n",
    "    parser.add_argument(\"--chunksize\", type=int, default=200_000)\n",
    "    parser.add_argument(\"--topk_categories\", type=int, default=8)\n",
    "    parser.add_argument(\"--k_clusters\", type=int, default=4)\n",
    "\n",
    "    # IMPORTANT: parse_known_args to ignore ipykernel -f ...json\n",
    "    args, _ = parser.parse_known_args(argv)\n",
    "\n",
    "    run_pipeline(\n",
    "        data_dir=args.data_dir,\n",
    "        state=args.state,\n",
    "        year=args.year,\n",
    "        chunksize=args.chunksize,\n",
    "        topk_categories=args.topk_categories,\n",
    "        k_clusters=args.k_clusters,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc3896-2e9f-414e-833f-52daaf4d81d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
